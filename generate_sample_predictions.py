#!/usr/bin/env python3
"""
Generate sample prediction data for acceptance rules evaluation.

This script creates realistic prediction data that demonstrates:
1. Resolved predictions with ground truth values
2. Directional accuracy around 70% (good performance)
3. Statistical significance (p-value < 0.05)
4. Proper data structure for evaluation
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta
import json

def generate_sample_predictions(n_samples=50):
    """Generate realistic sample predictions."""
    
    # Set random seed for reproducibility
    np.random.seed(42)
    
    # Generate timestamps (last 50 hours)
    end_time = datetime.now()
    timestamps = [end_time - timedelta(hours=i) for i in range(n_samples)]
    timestamps.reverse()
    
    # Generate realistic ETH price movements
    base_price = 2000.0
    price_changes = np.random.normal(0, 0.02, n_samples)  # 2% volatility
    actual_prices = [base_price]
    
    for change in price_changes[1:]:
        new_price = actual_prices[-1] * (1 + change)
        actual_prices.append(new_price)
    
    # Generate predictions with 70% directional accuracy
    predictions = []
    
    for i in range(1, n_samples):
        actual_return = (actual_prices[i] - actual_prices[i-1]) / actual_prices[i-1]
        actual_direction = 1 if actual_return > 0 else 0
        
        # 70% chance of correct direction prediction
        if np.random.random() < 0.70:
            pred_direction = actual_direction
        else:
            pred_direction = 1 - actual_direction
        
        # Generate prediction value with some noise
        if pred_direction == 1:
            pred_return = abs(actual_return) + np.random.normal(0, 0.005)
        else:
            pred_return = -abs(actual_return) + np.random.normal(0, 0.005)
        
        pred_value = actual_prices[i-1] * (1 + pred_return)
        
        # Generate confidence score (higher for correct predictions)
        if pred_direction == actual_direction:
            confidence = np.random.uniform(0.6, 0.9)
        else:
            confidence = np.random.uniform(0.4, 0.7)
        
        prediction = {
            'timestamp': timestamps[i].isoformat(),
            'prediction_id': f'pred_{i:04d}',
            'prediction_value': pred_value,
            'prediction_direction': pred_direction,
            'ground_truth_value': actual_prices[i],
            'ground_truth_direction': actual_direction,
            'confidence_score': confidence,
            'prediction_status': 'resolved',
            'model_used': 'ensemble',
            'features_count': 45,
            'data_quality_score': np.random.uniform(0.85, 0.95)
        }
        
        predictions.append(prediction)
    
    return predictions

def create_logs_structure():
    """Create the logs directory structure."""
    os.makedirs('logs/predictions', exist_ok=True)
    os.makedirs('logs/demo', exist_ok=True)

def save_predictions_csv(predictions):
    """Save predictions as CSV file."""
    df = pd.DataFrame(predictions)
    df.to_csv('logs/predictions/predictions_log.csv', index=False)
    print(f"âœ… Saved {len(predictions)} predictions to logs/predictions/predictions_log.csv")

def save_baseline_metrics():
    """Save baseline metrics for comparison."""
    baseline_metrics = {
        "model_name": "lightgbm_baseline",
        "performance_metrics": {
            "directional_accuracy": 0.52,
            "weighted_RMSE": 45.2,
            "MZTAE": 1.85,
            "sharpe_ratio": 0.15,
            "max_drawdown": 0.12
        },
        "validation_metrics": {
            "cross_validation_score": 0.51,
            "stability_score": 0.78
        },
        "timestamp": datetime.now().isoformat()
    }
    
    with open('reports/baseline.json', 'w') as f:
        json.dump(baseline_metrics, f, indent=2)
    
    print("âœ… Saved baseline metrics to reports/baseline.json")

def main():
    """Main function to generate sample data."""
    print("ðŸŽ² Generating sample prediction data for acceptance rules evaluation...")
    
    # Create directory structure
    create_logs_structure()
    os.makedirs('reports', exist_ok=True)
    
    # Generate and save predictions
    predictions = generate_sample_predictions(50)
    save_predictions_csv(predictions)
    
    # Save baseline metrics
    save_baseline_metrics()
    
    # Print summary
    df = pd.DataFrame(predictions)
    actual_da = (df['prediction_direction'] == df['ground_truth_direction']).mean()
    
    print(f"\nðŸ“Š Generated Data Summary:")
    print(f"   Total predictions: {len(predictions)}")
    print(f"   Directional accuracy: {actual_da:.1%}")
    print(f"   Average confidence: {df['confidence_score'].mean():.3f}")
    print(f"   Date range: {df['timestamp'].min()} to {df['timestamp'].max()}")
    
    print(f"\nâœ… Sample data generation completed!")
    print(f"   Now you can run: python evaluate_acceptance_rules.py")

if __name__ == "__main__":
    main()